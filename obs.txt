1-Ambiente:

* Utilizei um ambiente python onde instalei todas dependencias

python3 -m venv ambiente
source ambiente/bin/activate

2-Parâmetros:

- Alterações para análise de resultados -

* número de timesteps

model.learn(total_timesteps=int(200_000), progress_bar=True)

* taxa de aprendizado (learning_rate) 

model = DQN(...learning_rate=1e-3,...)

Um valor menor evita oscilações e melhora a convergência.
Um valor muito baixo pode fazer o aprendizado ser muito lento.


* fator de desconto (gamma) 

model = DQN(...gamma=0.99,...)

Valores mais altos (~0.99) priorizam recompensas futuras.
Valores mais baixos (~0.8) focam mais em recompensas imediatas.


* episódios na avaliação

mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)  

Isso garante que a avaliação não dependa de um único episódio e tenha uma métrica mais estável.



3-Saídas

* dqn_lunar_0 (primeira teste - padrão)

----------------------------------
| rollout/            |          |
|    ep_len_mean      | 829      |
|    ep_rew_mean      | 4.84     |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 328      |
|    fps              | 205      |
|    time_elapsed     | 970      |
|    total_timesteps  | 199128   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.474    |
|    n_updates        | 47281    |
----------------------------------

Recompensa média: -128.26 ± 73.87

timestep - 200_000




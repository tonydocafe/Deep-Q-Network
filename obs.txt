1-Ambiente:

* Utilizei um ambiente python onde instalei todas dependencias

python3 -m venv ambiente
source ambiente/bin/activate

2-Parâmetros:

- Alterações para análise de resultados -

* número de timesteps

model.learn(total_timesteps=int(200_000), progress_bar=True)

* taxa de aprendizado (learning_rate) 

model = DQN(...learning_rate=1e-3,...)

Um valor menor evita oscilações e melhora a convergência.
Um valor muito baixo pode fazer o aprendizado ser muito lento.


* fator de desconto (gamma) 

model = DQN(...gamma=0.99,...)

Valores mais altos (~0.99) priorizam recompensas futuras.
Valores mais baixos (~0.8) focam mais em recompensas imediatas.


* episódios na avaliação

mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10)  

Isso garante que a avaliação não dependa de um único episódio e tenha uma métrica mais estável.



3-Saídas

* dqn_lunar_0 (primeira teste - padrão)

----------------------------------
| rollout/            |          |
|    ep_len_mean      | 829      |
|    ep_rew_mean      | 4.84     |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 328      |
|    fps              | 205      |
|    time_elapsed     | 970      |
|    total_timesteps  | 199128   |
| train/              |          |
|    learning_rate    | 0.001    |
|    loss             | 0.474    |
|    n_updates        | 47281    |
----------------------------------

Recompensa média: -128.26 ± 73.87

timestep - 200_000
learning_rate - 1e-3(1 * 10^(-3) = 0.001)
gamma - 0.99 
n_eval_episodes - 10 



* dqn_lunar_1 (segundo teste - amplo)

----------------------------------
| rollout/            |          |
|    ep_len_mean      | 238      |
|    ep_rew_mean      | -381     |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 1800     |
|    fps              | 323      |
|    time_elapsed     | 1540     |
|    total_timesteps  | 498805   |
| train/              |          |
|    learning_rate    | 0.005    |
|    loss             | 3.26     |
|    n_updates        | 122201   |
----------------------------------

Recompensa média: -364.51 ± 50.96


timestep - 500_000
learning_rate - 5e-3 (5 * 10^(-3) = 0.005)
gamma - 0.999 
n_eval_episodes - 50 

* dqn_lunar_2 (terceiro teste - definido)

----------------------------------
| rollout/            |          |
|    ep_len_mean      | 390      |
|    ep_rew_mean      | -246     |
|    exploration_rate | 0.05     |
| time/               |          |
|    episodes         | 200      |
|    fps              | 323      |
|    time_elapsed     | 150      |
|    total_timesteps  | 48687    |
| train/              |          |
|    learning_rate    | 0.0001   |
|    loss             | 0.914    |
|    n_updates        | 9671     |
----------------------------------

Recompensa média: -91.90 ± 29.75


timestep - 50_000
learning_rate - 1e-4 (1 * 10^(-4) = 0.0001)
gamma - 0.85 
n_eval_episodes - 5 
